# 对实验的思考

  ~~本次修改是基于之前`cosine+softmax`余弦相似度定义权重做的。现在改成`euler_dist+normalize`来做。初步判断效果应该是差不多的。~~

  初步判断，之前实验涨点的关还有什么更优的组合方式。

  根据`#122`的实验结果以及公式的分析，这种改善的方式应该不是什么比较关键的。从逻辑上来说等于，先定义一个权值为`1/T`的权重向量，然后再用这个向量再来引导新的权重向量的生成。这个`idea`感觉不是很`make sense`。还是用回原来的办法吧，思考以下原来那个办法的权重能否做另一方面的改进。

# 实验说明

  我们计算特征向量的核心公式为：
$$
F_{ij}^{n+1} = (\alpha \times f_i^n + \beta \times f_j^n)^2 + \gamma \times f_i^n + \delta \times f_j^n
$$

  之前的$ \alpha $和$ \beta$
$$
\alpha = ReLU\Big( W_{2,i}\times Sigmoid( W_{1,i}\times f_j^n)\Big)
$$
$$
\beta = ReLU\big( W_{2,j}\times Sigmoid( W_{2,j}\times f_i^n)\big)
$$
但是存在的缺陷是，$\alpha$的生成只和$f_i^n$相对的向量$f_j^n$有关，而和自己本身$f_i^n$无关。虽然，这样看上去比较高端。但是，逻辑上是有点讲不通的。用`对方`通道的特征来决定其他通道的特征？感觉上，并不`make sense`。无论做什么操作，都应该找到这么做的理论参考依据，这才是实验的价值。

  所以，针对这个情况，我对实验进行了调整。即，
$$
\alpha = Relu\Big\{W_{2,i}\times \big[ Sigmoid( W_{1,i}\times f_i^n ) + Sigmoid( W_{1,j})\times f_j^n \big]\Big\}
$$
$$
\beta = Relu\Big\{W_{2,j}\times \big[ Sigmoid( W_{1,i}\times f_i^n ) + Sigmoid( W_{1,j})\times f_j^n \big]\Big\}
$$
用加法把这两个特征权向量组合起来，再分别用不同的全连接层得到对应特征图的权向量。

# 实验结果

|epoch|mAP|Rank1|Rank5|
|:--:|:--:|:--:|:--:|
|520|80.8%|86.4%|95.2%|
|480|80.7%|86.7%|95.1%|
|440|80.6%|86.4%|95.3%|
|400|80.6%|86.2%|95.1%|
|360|80.4%|86.3%|95.0%|

# 结果分析

## 实验特点
1.和没有作更改相比，性能出现下滑。
2.训练过程中，`交叉熵`的损失函数在逼近`15`以后不再下降；`三元组`的损失函数在逼近`0.6`以后不再下降。==这是一个奇怪但又很有意思的事情，如果找到原因，有可能是一个不错的突破。==通过打印各阶对应的`交叉熵`和`三元组`的值来观察这中间的变化。看是不是能通过添加超参数来缓解这个问题。

## 分析

  直接无权重的相加，感觉上确实不是很`work`。感觉只是一个能让程序运行的权宜之计。重点是，要讨论和定义这个生成出来的`权向量`的意义。再根据这个背景和意义来判断计算的办法是否合理，存在什么改进的空间。

  生成的`权向量`是用来提取相邻两帧特征图的`动作语义`的。即，需要放过两个特征图中不变的部分，捕捉变化的部分，用神经网络找到一个`合理`的函数来表示这个`变化部分`对应的特征。那采用`SENet`的思路好像并没有做到这一点。`SENet`的目的只是近一步考虑通道于通道之间的关系，然后根据这个通道的关系，生成对应的`权向量`，加强特征图的表示。

  结论是，本次实验中没有体现我们处理数据的思想，只是实现了一个可以运行的实验。还需要好好思考以下，==怎么把`SENet`的思想迁移到我们考虑`动作语义`的思想上来==。

#  需注意这个实验本事存在问题！！ 好好理解以下`FC`的计算方式！！















